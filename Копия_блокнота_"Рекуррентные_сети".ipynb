{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Копия блокнота \"Рекуррентные сети\"",
      "provenance": [],
      "mount_file_id": "1nEFNWzCiONfNe7EsvGFi8IF3DMIF0mlS",
      "authorship_tag": "ABX9TyNtTSIIsM2Lbpy5T59ggzvf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flowergum/NN2/blob/RNN/%D0%9A%D0%BE%D0%BF%D0%B8%D1%8F_%D0%B1%D0%BB%D0%BE%D0%BA%D0%BD%D0%BE%D1%82%D0%B0_%22%D0%A0%D0%B5%D0%BA%D1%83%D1%80%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B5_%D1%81%D0%B5%D1%82%D0%B8%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WHgIKod_Dbo"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-2ENiy-_Oas"
      },
      "source": [
        "path_to_file = \"drive/My Drive/Recurrent_NN/input/frankenstein.txt\"\n",
        "\n",
        "# Открыть файл на чтение в бинарном виде, считать и декодировать\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG3Ta4oXAU1y"
      },
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  try:\n",
        "    # Currently, memory growth needs to be the same across GPUs\n",
        "    for gpu in gpus:\n",
        "      tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "  except RuntimeError as e:\n",
        "    # Memory growth must be set before GPUs have been initialized\n",
        "    print(e)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dcKpvcAAdW8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e76ce3d1-cb54-49f4-e14e-18275840ebff"
      },
      "source": [
        "def check_text(text):\n",
        "    # Посмотрим первые 300 символов\n",
        "    print(text[:300])\n",
        "    # Посмотрим общее количество символов\n",
        "    print ('\\nLength of text: {} characters'.format(len(text)))\n",
        "    # Построим перечень уникальных символов, а numpy поможет нам работать с массивом\n",
        "    vocab = np.array(sorted(set(text)))\n",
        "    # Выведем на экран все уникальные символы\n",
        "    print ('{} unique characters:'.format(len(vocab)))\n",
        "    print(vocab)\n",
        "    # Нестандартные специальные символы в английском начинаются после символа 'z'\n",
        "    print ('Bad characters in english text:')\n",
        "    print(vocab[vocab > 'z'])\n",
        "    return vocab\n",
        "\n",
        "vocab = check_text(text)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿Letter 1\r\n",
            "\r\n",
            "_To Mrs. Saville, England._\r\n",
            "\r\n",
            "\r\n",
            "St. Petersburgh, Dec. 11th, 17—.\r\n",
            "\r\n",
            "\r\n",
            "You will rejoice to hear that no disaster has accompanied the\r\n",
            "commencement of an enterprise which you have regarded with such evil\r\n",
            "forebodings.  I arrived here yesterday, and my first task is to assure\r\n",
            "my dear sis\n",
            "\n",
            "Length of text: 428004 characters\n",
            "85 unique characters:\n",
            "['\\n' '\\r' ' ' '!' '(' ')' ',' '-' '.' '0' '1' '2' '3' '4' '5' '6' '7' '8'\n",
            " '9' ':' ';' '?' 'A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N'\n",
            " 'O' 'P' 'R' 'S' 'T' 'U' 'V' 'W' 'Y' '[' ']' '_' 'a' 'b' 'c' 'd' 'e' 'f'\n",
            " 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o' 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x'\n",
            " 'y' 'z' 'æ' 'è' 'é' 'ê' 'ô' '—' '‘' '’' '“' '”' '\\ufeff']\n",
            "Bad characters in english text:\n",
            "['æ' 'è' 'é' 'ê' 'ô' '—' '‘' '’' '“' '”' '\\ufeff']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gURP3t6tAll8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f6fc1df-a138-44e3-9220-f4e32329945e"
      },
      "source": [
        "# Удаляем символы после последнего значимого\n",
        "text = text.translate({ord(c): None for c in vocab[vocab > 'z']}) # As recommended in https://stackoverflow.com/questions/3939361/remove-specific-characters-from-a-string-in-python\n",
        "# Снова смотрим на текст, на словарь и пересохраняем его себе\n",
        "vocab = check_text(text)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Letter 1\r\n",
            "\r\n",
            "_To Mrs. Saville, England._\r\n",
            "\r\n",
            "\r\n",
            "St. Petersburgh, Dec. 11th, 17.\r\n",
            "\r\n",
            "\r\n",
            "You will rejoice to hear that no disaster has accompanied the\r\n",
            "commencement of an enterprise which you have regarded with such evil\r\n",
            "forebodings.  I arrived here yesterday, and my first task is to assure\r\n",
            "my dear siste\n",
            "\n",
            "Length of text: 426888 characters\n",
            "74 unique characters:\n",
            "['\\n' '\\r' ' ' '!' '(' ')' ',' '-' '.' '0' '1' '2' '3' '4' '5' '6' '7' '8'\n",
            " '9' ':' ';' '?' 'A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N'\n",
            " 'O' 'P' 'R' 'S' 'T' 'U' 'V' 'W' 'Y' '[' ']' '_' 'a' 'b' 'c' 'd' 'e' 'f'\n",
            " 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o' 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x'\n",
            " 'y' 'z']\n",
            "Bad characters in english text:\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQLTTLePAxjd"
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = vocab\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text]).astype('uint8')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9lmDyDJA265",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f32bf5a9-5cc3-4e18-9522-c6c6bdd60154"
      },
      "source": [
        "# Show how the first 13 characters from the text are mapped to integers\n",
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Letter 1\\r\\n\\r\\n_' ---- characters mapped to int ---- > [33 52 67 67 52 65  2 10  1  0  1  0 47]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY4_8emsBJ6T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc30b9ab-b460-45f5-bdbf-12252b3d3574"
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "print(\"There will be {} examples\".format(examples_per_epoch))\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "print(\"Let's check tensor properties: {}\".format(char_dataset))\n",
        "\n",
        "# Enumerate method\n",
        "for i,v in char_dataset.enumerate().take(7):\n",
        "    print (idx2char[v.numpy()])\n",
        "# Simplier method\n",
        "for v in char_dataset.take(7):\n",
        "    print (idx2char[v.numpy()])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There will be 4226 examples\n",
            "Let's check tensor properties: <TensorSliceDataset shapes: (), types: tf.uint8>\n",
            "L\n",
            "e\n",
            "t\n",
            "t\n",
            "e\n",
            "r\n",
            " \n",
            "L\n",
            "e\n",
            "t\n",
            "t\n",
            "e\n",
            "r\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1kNZmaGBRVR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23e0212e-eb83-45e1-9913-b2c70546b391"
      },
      "source": [
        "# Slicing dataset\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "# Now we can check the properties\n",
        "print(\"Let's check tensor properties: {}\".format(sequences))\n",
        "\n",
        "# Visualize first 2 sequences translating them to letters\n",
        "for item in sequences.take(2):\n",
        "    print (idx2char[item.numpy()])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Let's check tensor properties: <BatchDataset shapes: (101,), types: tf.uint8>\n",
            "['L' 'e' 't' 't' 'e' 'r' ' ' '1' '\\r' '\\n' '\\r' '\\n' '_' 'T' 'o' ' ' 'M'\n",
            " 'r' 's' '.' ' ' 'S' 'a' 'v' 'i' 'l' 'l' 'e' ',' ' ' 'E' 'n' 'g' 'l' 'a'\n",
            " 'n' 'd' '.' '_' '\\r' '\\n' '\\r' '\\n' '\\r' '\\n' 'S' 't' '.' ' ' 'P' 'e' 't'\n",
            " 'e' 'r' 's' 'b' 'u' 'r' 'g' 'h' ',' ' ' 'D' 'e' 'c' '.' ' ' '1' '1' 't'\n",
            " 'h' ',' ' ' '1' '7' '.' '\\r' '\\n' '\\r' '\\n' '\\r' '\\n' 'Y' 'o' 'u' ' ' 'w'\n",
            " 'i' 'l' 'l' ' ' 'r' 'e' 'j' 'o' 'i' 'c' 'e' ' ' 't' 'o']\n",
            "[' ' 'h' 'e' 'a' 'r' ' ' 't' 'h' 'a' 't' ' ' 'n' 'o' ' ' 'd' 'i' 's' 'a'\n",
            " 's' 't' 'e' 'r' ' ' 'h' 'a' 's' ' ' 'a' 'c' 'c' 'o' 'm' 'p' 'a' 'n' 'i'\n",
            " 'e' 'd' ' ' 't' 'h' 'e' '\\r' '\\n' 'c' 'o' 'm' 'm' 'e' 'n' 'c' 'e' 'm' 'e'\n",
            " 'n' 't' ' ' 'o' 'f' ' ' 'a' 'n' ' ' 'e' 'n' 't' 'e' 'r' 'p' 'r' 'i' 's'\n",
            " 'e' ' ' 'w' 'h' 'i' 'c' 'h' ' ' 'y' 'o' 'u' ' ' 'h' 'a' 'v' 'e' ' ' 'r'\n",
            " 'e' 'g' 'a' 'r' 'd' 'e' 'd' ' ' 'w' 'i' 't']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHUDyNSjBYFP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b9f8188-29f7-47c7-abf8-f992574ccd40"
      },
      "source": [
        "def show_str(item, prefix=None):\n",
        "    if prefix is not None:\n",
        "       print(prefix)\n",
        "    print(\"\".join(idx2char[item]))\n",
        "\n",
        "# Visualize first 3 sequences\n",
        "for i,v in sequences.enumerate().take(3):\n",
        "    show_str(v, \"Sequence {}:\".format(i))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequence 0:\n",
            "Letter 1\r\n",
            "\r\n",
            "_To Mrs. Saville, England._\r\n",
            "\r\n",
            "\r\n",
            "St. Petersburgh, Dec. 11th, 17.\r\n",
            "\r\n",
            "\r\n",
            "You will rejoice to\n",
            "Sequence 1:\n",
            " hear that no disaster has accompanied the\r\n",
            "commencement of an enterprise which you have regarded wit\n",
            "Sequence 2:\n",
            "h such evil\r\n",
            "forebodings.  I arrived here yesterday, and my first task is to assure\r\n",
            "my dear sister o\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rk2sYxF4Eo-G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec4b3101-ccb4-422d-905d-c97d1e3e3482"
      },
      "source": [
        "# Define splitting method\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "# Test splitting method\n",
        "print(split_input_target(\"Test phrase\"))\n",
        "\n",
        "# Creating dataset from sequences with our splitting method\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Test phras', 'est phrase')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YoqZfOaE153",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c045e23-232b-41f7-c3d2-a94909867b4d"
      },
      "source": [
        "for input_example, target_example in dataset.take(2):\n",
        "    show_str (input_example, 'Input data: ')\n",
        "    show_str (target_example, 'Target data: ')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data: \n",
            "Letter 1\r\n",
            "\r\n",
            "_To Mrs. Saville, England._\r\n",
            "\r\n",
            "\r\n",
            "St. Petersburgh, Dec. 11th, 17.\r\n",
            "\r\n",
            "\r\n",
            "You will rejoice t\n",
            "Target data: \n",
            "etter 1\r\n",
            "\r\n",
            "_To Mrs. Saville, England._\r\n",
            "\r\n",
            "\r\n",
            "St. Petersburgh, Dec. 11th, 17.\r\n",
            "\r\n",
            "\r\n",
            "You will rejoice to\n",
            "Input data: \n",
            " hear that no disaster has accompanied the\r\n",
            "commencement of an enterprise which you have regarded wi\n",
            "Target data: \n",
            "hear that no disaster has accompanied the\r\n",
            "commencement of an enterprise which you have regarded wit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wPX-QXpE8T3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7053fe8a-c0b8-4a7a-a21f-f9b5e362bd9a"
      },
      "source": [
        "# Take first dataset string pair again\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    pass\n",
        "# Show symbols for each time step as illustration\n",
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, idx2char[input_idx]))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, idx2char[target_idx]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 33 (L)\n",
            "  expected output: 52 (e)\n",
            "Step    1\n",
            "  input: 52 (e)\n",
            "  expected output: 67 (t)\n",
            "Step    2\n",
            "  input: 67 (t)\n",
            "  expected output: 67 (t)\n",
            "Step    3\n",
            "  input: 67 (t)\n",
            "  expected output: 52 (e)\n",
            "Step    4\n",
            "  input: 52 (e)\n",
            "  expected output: 65 (r)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SCCer7aE_f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f485e1ec-7f48-41fe-c957-071b6a0c65ae"
      },
      "source": [
        "# Check dataset classes\n",
        "print(\"dataset class is {}\".format(dataset))\n",
        "print(\"dataset is a child of tf.data.Dataset? {}.\".format(isinstance(dataset, tf.data.Dataset)))\n",
        "\n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset_batched = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "print(\"dataset_batched is new variable of class {}.\".format(dataset_batched))\n",
        "print(\"dataset_batched is a child of tf.data.Dataset? {}.\".format(isinstance(dataset_batched, tf.data.Dataset)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset class is <MapDataset shapes: ((100,), (100,)), types: (tf.uint8, tf.uint8)>\n",
            "dataset is a child of tf.data.Dataset? True.\n",
            "dataset_batched is new variable of class <BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.uint8, tf.uint8)>.\n",
            "dataset_batched is a child of tf.data.Dataset? True.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nduES9hcFDov",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c0e5098-9375-47a8-fbda-f97068021d8f"
      },
      "source": [
        "# Take first dataset batch as a pair\n",
        "for input_batch, target_batch in dataset_batched.take(1):\n",
        "    pass\n",
        "\n",
        "# You can check that we have 2D array with the following debug output\n",
        "print(\"input_batch is new variable of class {}.\".format(input_batch))\n",
        "print(\"So dataset_batched can be converted to a pair of arrays with shape {} and {}.\".format(input_batch.shape, target_batch.shape))\n",
        "\n",
        "# Let's look into some strings\n",
        "for i in range(2):\n",
        "    input_example = input_batch[i]\n",
        "    target_example = target_batch[i]\n",
        "    print(\"String pair #{}\".format(i))\n",
        "    show_str(input_example, \"Input sequence:\")\n",
        "    show_str(target_example, \"Target sequence:\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_batch is new variable of class [[61 56 61 ...  2 48  2]\n",
            " [ 8  2  2 ... 30  2 51]\n",
            " [ 2 55 48 ...  2 55 52]\n",
            " ...\n",
            " [61  2 70 ...  2 67 62]\n",
            " [56 60 60 ... 56 61 58]\n",
            " [56 61 51 ... 51  6  2]].\n",
            "So dataset_batched can be converted to a pair of arrays with shape (64, 100) and (64, 100).\n",
            "String pair #0\n",
            "Input sequence:\n",
            "ning with the swiftness of lightning,\r\n",
            "plunged into the lake.\r\n",
            "\r\n",
            "The report of the pistol brought a \n",
            "Target sequence:\n",
            "ing with the swiftness of lightning,\r\n",
            "plunged into the lake.\r\n",
            "\r\n",
            "The report of the pistol brought a c\n",
            "String pair #1\n",
            "Input sequence:\n",
            ".  Some hours passed thus, while they, by their countenances,\r\n",
            "expressed joy, the cause of which I d\n",
            "Target sequence:\n",
            "  Some hours passed thus, while they, by their countenances,\r\n",
            "expressed joy, the cause of which I di\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWdbZh6-FIbe"
      },
      "source": [
        "for input_batch, target_batch in dataset_batched.take(1):\n",
        "    pass\n",
        "#tf.one_hot(input_batch, len(vocab))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGJUk_VQFOOP"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Number of RNN units\n",
        "lstm_units = 1024\n",
        "\n",
        "def build_model(vocab_size, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    # To call tf.one_hot() we use Lambda, but we also have to cast type to uint8 \n",
        "    # because otherwise model weights can't be imported as they do not support default float32 type.\n",
        "    # We also have to define batch shape because model can't be used wighout knowing its input shape.\n",
        "    # It can be defined on first use, but it is more simple to define it now.\n",
        "    tf.keras.layers.Embedding(input_dim = len(vocab), output_dim = vocab_size, batch_input_shape = [batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwBgGWJvFT4X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cba008a2-f96d-4eff-92c5-254bd6c5bd3a"
      },
      "source": [
        "print(BATCH_SIZE)\n",
        "model_batched = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  rnn_units=lstm_units,\n",
        "  batch_size=BATCH_SIZE)\n",
        "\n",
        "model_batched.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 74)            5476      \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (64, None, 1024)          4501504   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 74)            75850     \n",
            "=================================================================\n",
            "Total params: 4,582,830\n",
            "Trainable params: 4,582,830\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gigBPtXmFYp-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "580f7e9b-3846-4125-9d7f-14690fc840d7"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset_batched.take(1):\n",
        "  print(\"Input shape \", input_example_batch.shape, \"# (batch_size, sequence_length)\")\n",
        "  print(\"Target shape \", target_example_batch.shape, \"# (batch_size, sequence_length)\")\n",
        "  example_batch_predictions = model_batched(input_example_batch)\n",
        "  print(\"Prediction shape \", example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input shape  (64, 100) # (batch_size, sequence_length)\n",
            "Target shape  (64, 100) # (batch_size, sequence_length)\n",
            "Prediction shape  (64, 100, 74) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH1nSntWFcfu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91f800c7-5cd4-4720-8d0c-943b00616cf6"
      },
      "source": [
        "sampled_indices = tf.argmax(example_batch_predictions[0], axis=1)\n",
        "print(sampled_indices)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[50 28 42 42  6  6 46 35 45 46 46 42  5  5  5  6  9 46 46 13 46  7  4  1\n",
            " 46 46 35 11 63 63 63  9 52 14 14  9 45 45 65 35 35 45 45 45 45 21 45 42\n",
            " 42 24 45 45 45 45 45 45  6  1 45 45  6 13 22 13 14  4 14 28 14 14 14 45\n",
            " 42 42 60 42 18 61 11  5 45 45 45 45 28 42 46  4 14 14 14 14  4 45 45 45\n",
            " 45 45 45 45], shape=(100,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxwOPTITFfWf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2fffd37-5193-49d2-a4d2-e1db63af8c03"
      },
      "source": [
        "show_str(input_example_batch[0], \"Input sequence:\")\n",
        "show_str(target_example_batch[0], \"Target sequence:\")\n",
        "show_str(sampled_indices, \"Predicted sequence:\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input sequence:\n",
            "ith more than\r\n",
            "mortal speed.\r\n",
            "\r\n",
            "I pursued him, and for many months this has been my task.  Guided by\n",
            "Target sequence:\n",
            "th more than\r\n",
            "mortal speed.\r\n",
            "\r\n",
            "I pursued him, and for many months this has been my task.  Guided by \n",
            "Predicted sequence:\n",
            "cGVV,,]N[]]V))),0]]4]-(\r]]N2ppp0e550[[rNN[[[[?[VVC[[[[[[,\r[[,4A45(5G555[VVmV9n2)[[[[GV](5555([[[[[[[\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXUHFEuZFk_S"
      },
      "source": [
        "model_batched.compile(optimizer='adam', loss=tf.keras.losses.sparse_categorical_crossentropy)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCLAf3szFn9p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ce3061c-c64b-449b-a087-49195ef10f81"
      },
      "source": [
        "example_batch_loss  = tf.keras.losses.sparse_categorical_crossentropy(target_example_batch, example_batch_predictions)\n",
        "print(\"scalar_loss: \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "scalar_loss:  4.3051033\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L5GdYwrFrxJ"
      },
      "source": [
        "EPOCHS=10"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sOmb7b5FsTL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c066c597-8b44-44af-e812-db6560e15338"
      },
      "source": [
        "history = model_batched.fit(dataset_batched, epochs=EPOCHS)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "66/66 [==============================] - 399s 6s/step - loss: 3.0972\n",
            "Epoch 2/10\n",
            "66/66 [==============================] - 399s 6s/step - loss: 2.5984\n",
            "Epoch 3/10\n",
            "66/66 [==============================] - 404s 6s/step - loss: 2.3230\n",
            "Epoch 4/10\n",
            "66/66 [==============================] - 404s 6s/step - loss: 2.1761\n",
            "Epoch 5/10\n",
            "66/66 [==============================] - 402s 6s/step - loss: 2.0481\n",
            "Epoch 6/10\n",
            "66/66 [==============================] - 402s 6s/step - loss: 1.9397\n",
            "Epoch 7/10\n",
            "66/66 [==============================] - 401s 6s/step - loss: 1.8452\n",
            "Epoch 8/10\n",
            "66/66 [==============================] - 403s 6s/step - loss: 1.7651\n",
            "Epoch 9/10\n",
            "66/66 [==============================] - 401s 6s/step - loss: 1.6957\n",
            "Epoch 10/10\n",
            "66/66 [==============================] - 403s 6s/step - loss: 1.6280\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrwcPEfEFwY1"
      },
      "source": [
        "model_weights = model_batched.get_weights()\n",
        "\n",
        "model_single = build_model(vocab_size, lstm_units, batch_size=1)\n",
        "\n",
        "model_single.set_weights(model_weights)\n",
        "\n",
        "model_single.build()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzE-g9ywFyY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff3ed94c-b64a-430d-8c09-f92fd5f1995d"
      },
      "source": [
        "model_single.summary()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 74)             5476      \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (1, None, 1024)           4501504   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 74)             75850     \n",
            "=================================================================\n",
            "Total params: 4,582,830\n",
            "Trainable params: 4,582,830\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eV1NzGn1F1N3"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = np.array([char2idx[s] for s in start_string])\n",
        "  print(\"input_eval original shape is \", input_eval.shape)\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "  print(\"input_eval shape changed to \", input_eval.shape)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Here batch size == 1\n",
        "  # We reset the memory of RNN so it will forget the previous timeline os characters sequence\n",
        "  model.reset_states()\n",
        "    \n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # print(\"Predictions shape before squeezing \", predictions.shape)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "      # print(\"Predictions shape after squeezing \", predictions.shape)\n",
        "\n",
        "      # using argmax() to determine next symbol\n",
        "      #predicted_id = tf.argmax(predictions[0])\n",
        "\n",
        "      temperature = 0.021\n",
        "      predicted_id = tf.random.categorical(predictions / temperature, num_samples=1)[-1,0].numpy()\n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JcoBlx7F3ZH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e4146df-2da0-45f2-d744-c907bcdff716"
      },
      "source": [
        "print(generate_text(model_single, start_string=u\"We are on the verge of \"))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_eval original shape is  (23,)\n",
            "input_eval shape changed to  (1, 23)\n",
            "We are on the verge of my father of the most of the remanter of the occupation of my father of the courtent of my father of the 5ccupation of my hands of the \n",
            "concention of the Marseration of the process of the most of the could Victor of my father of the sume of the sume of my unerest the could which I had not heart was so the into the could Lather of my the courtent of the sume of my father of the could not heart and searour the could zassed the same of my hands of the words of my same the first which I had not the could of the Hoors of my father of the Light of a surrow me the mounter of the courtent of the surrow of my nears of my father of the country which I had been the same some the could sand the same of my father of the courtent of the country of the could Cleaved the could some Justine Before the same of the could of the country which had concented the could not suppress of my father of my suppose the sume of the could concent of the sumpose of my father of the same of the courtent of the could n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx8B82L9F-Ci"
      },
      "source": [
        "Эксперимент 1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x1gq-19F_Y_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5dc674a-7d90-488b-f7cc-62fb054a4c9c"
      },
      "source": [
        "num_of_samples = 200\n",
        "probabilities = np.array([[0.5, 0.2, 0.2, 0.1]])\n",
        "temperature = 1\n",
        "\n",
        "samples = tf.random.categorical(tf.math.log(probabilities) / temperature, num_of_samples)\n",
        "print(samples)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[3 1 2 0 2 0 1 1 0 3 0 0 0 0 1 0 0 2 2 0 0 0 0 3 2 1 1 1 2 0 0 2 0 2 3 0\n",
            "  2 0 0 3 0 0 0 1 1 0 0 0 2 0 3 0 1 2 3 0 3 1 0 1 3 0 0 3 0 0 3 1 2 0 0 1\n",
            "  0 3 0 1 1 0 0 0 2 2 0 0 1 1 0 1 2 1 0 2 2 0 2 3 1 0 2 3 0 1 1 0 0 2 1 2\n",
            "  1 2 0 0 0 0 0 0 2 0 1 0 0 3 3 1 0 2 0 0 1 2 0 1 0 2 0 0 2 1 0 1 1 0 1 0\n",
            "  1 1 0 0 0 0 1 1 0 3 1 2 2 1 1 2 3 0 0 0 0 0 2 2 2 0 2 1 1 1 0 3 0 0 0 0\n",
            "  0 1 0 3 0 0 3 1 0 1 3 0 0 0 1 0 0 1 0 0]], shape=(1, 200), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDz4RfWaGFe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "f00bd45b-8501-4c41-e38c-d42d1cbfb7e0"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# the histogram of the data\n",
        "_, bins, _ = plt.hist(samples, bins=len(probabilities[0]), \n",
        "                            density=False, color='lawngreen', rwidth = 0.8)\n",
        "# Probability lines\n",
        "for p in probabilities[0]:\n",
        "    plt.plot(bins, [p*num_of_samples for _ in bins], color='b' )\n",
        "plt.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}