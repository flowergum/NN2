{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Рекуррентные сети",
      "provenance": [],
      "mount_file_id": "1G3dTtEjpbzF-oXcHV4JuEh2jRLSJd77W",
      "authorship_tag": "ABX9TyP2NbCmqUG979vjkkF8lRF1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flowergum/NN2/blob/main/%D0%A0%D0%B5%D0%BA%D1%83%D1%80%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B5_%D1%81%D0%B5%D1%82%D0%B8_one_hot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WHgIKod_Dbo"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-2ENiy-_Oas"
      },
      "source": [
        "path_to_file = \"drive/My Drive/Recurrent_NN/input/frankenstein.txt\"\n",
        "\n",
        "# Открыть файл на чтение в бинарном виде, считать и декодировать\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG3Ta4oXAU1y"
      },
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  try:\n",
        "    # Currently, memory growth needs to be the same across GPUs\n",
        "    for gpu in gpus:\n",
        "      tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "  except RuntimeError as e:\n",
        "    # Memory growth must be set before GPUs have been initialized\n",
        "    print(e)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dcKpvcAAdW8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "831e7821-d27b-4588-f78f-b327ecaa660c"
      },
      "source": [
        "def check_text(text):\n",
        "    # Посмотрим первые 300 символов\n",
        "    print(text[:300])\n",
        "    # Посмотрим общее количество символов\n",
        "    print ('\\nLength of text: {} characters'.format(len(text)))\n",
        "    # Построим перечень уникальных символов, а numpy поможет нам работать с массивом\n",
        "    vocab = np.array(sorted(set(text)))\n",
        "    # Выведем на экран все уникальные символы\n",
        "    print ('{} unique characters:'.format(len(vocab)))\n",
        "    print(vocab)\n",
        "    # Нестандартные специальные символы в английском начинаются после символа 'z'\n",
        "    print ('Bad characters in english text:')\n",
        "    print(vocab[vocab > 'z'])\n",
        "    return vocab\n",
        "\n",
        "vocab = check_text(text)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿Letter 1\r\n",
            "\r\n",
            "_To Mrs. Saville, England._\r\n",
            "\r\n",
            "\r\n",
            "St. Petersburgh, Dec. 11th, 17—.\r\n",
            "\r\n",
            "\r\n",
            "You will rejoice to hear that no disaster has accompanied the\r\n",
            "commencement of an enterprise which you have regarded with such evil\r\n",
            "forebodings.  I arrived here yesterday, and my first task is to assure\r\n",
            "my dear sis\n",
            "\n",
            "Length of text: 428004 characters\n",
            "85 unique characters:\n",
            "['\\n' '\\r' ' ' '!' '(' ')' ',' '-' '.' '0' '1' '2' '3' '4' '5' '6' '7' '8'\n",
            " '9' ':' ';' '?' 'A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N'\n",
            " 'O' 'P' 'R' 'S' 'T' 'U' 'V' 'W' 'Y' '[' ']' '_' 'a' 'b' 'c' 'd' 'e' 'f'\n",
            " 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o' 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x'\n",
            " 'y' 'z' 'æ' 'è' 'é' 'ê' 'ô' '—' '‘' '’' '“' '”' '\\ufeff']\n",
            "Bad characters in english text:\n",
            "['æ' 'è' 'é' 'ê' 'ô' '—' '‘' '’' '“' '”' '\\ufeff']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gURP3t6tAll8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3967ff8-906f-4e64-86a9-198e69c7d90d"
      },
      "source": [
        "# Удаляем символы после последнего значимого\n",
        "text = text.translate({ord(c): None for c in vocab[vocab > 'z']}) # As recommended in https://stackoverflow.com/questions/3939361/remove-specific-characters-from-a-string-in-python\n",
        "# Снова смотрим на текст, на словарь и пересохраняем его себе\n",
        "vocab = check_text(text)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Letter 1\r\n",
            "\r\n",
            "_To Mrs. Saville, England._\r\n",
            "\r\n",
            "\r\n",
            "St. Petersburgh, Dec. 11th, 17.\r\n",
            "\r\n",
            "\r\n",
            "You will rejoice to hear that no disaster has accompanied the\r\n",
            "commencement of an enterprise which you have regarded with such evil\r\n",
            "forebodings.  I arrived here yesterday, and my first task is to assure\r\n",
            "my dear siste\n",
            "\n",
            "Length of text: 426888 characters\n",
            "74 unique characters:\n",
            "['\\n' '\\r' ' ' '!' '(' ')' ',' '-' '.' '0' '1' '2' '3' '4' '5' '6' '7' '8'\n",
            " '9' ':' ';' '?' 'A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N'\n",
            " 'O' 'P' 'R' 'S' 'T' 'U' 'V' 'W' 'Y' '[' ']' '_' 'a' 'b' 'c' 'd' 'e' 'f'\n",
            " 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o' 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x'\n",
            " 'y' 'z']\n",
            "Bad characters in english text:\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQLTTLePAxjd"
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = vocab\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text]).astype('uint8')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9lmDyDJA265",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2fb927d-ca21-4f01-a582-59ef2ee5ed88"
      },
      "source": [
        "# Show how the first 13 characters from the text are mapped to integers\n",
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Letter 1\\r\\n\\r\\n_' ---- characters mapped to int ---- > [33 52 67 67 52 65  2 10  1  0  1  0 47]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY4_8emsBJ6T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e98dd08-601d-4028-f505-e23b9ba61cb6"
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "print(\"There will be {} examples\".format(examples_per_epoch))\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "print(\"Let's check tensor properties: {}\".format(char_dataset))\n",
        "\n",
        "# Enumerate method\n",
        "for i,v in char_dataset.enumerate().take(7):\n",
        "    print (idx2char[v.numpy()])\n",
        "# Simplier method\n",
        "for v in char_dataset.take(7):\n",
        "    print (idx2char[v.numpy()])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There will be 4226 examples\n",
            "Let's check tensor properties: <TensorSliceDataset shapes: (), types: tf.uint8>\n",
            "L\n",
            "e\n",
            "t\n",
            "t\n",
            "e\n",
            "r\n",
            " \n",
            "L\n",
            "e\n",
            "t\n",
            "t\n",
            "e\n",
            "r\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1kNZmaGBRVR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c1ea6e6-c14c-42f9-bc91-470a15c14f92"
      },
      "source": [
        "# Slicing dataset\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "# Now we can check the properties\n",
        "print(\"Let's check tensor properties: {}\".format(sequences))\n",
        "\n",
        "# Visualize first 2 sequences translating them to letters\n",
        "for item in sequences.take(2):\n",
        "    print (idx2char[item.numpy()])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Let's check tensor properties: <BatchDataset shapes: (101,), types: tf.uint8>\n",
            "['L' 'e' 't' 't' 'e' 'r' ' ' '1' '\\r' '\\n' '\\r' '\\n' '_' 'T' 'o' ' ' 'M'\n",
            " 'r' 's' '.' ' ' 'S' 'a' 'v' 'i' 'l' 'l' 'e' ',' ' ' 'E' 'n' 'g' 'l' 'a'\n",
            " 'n' 'd' '.' '_' '\\r' '\\n' '\\r' '\\n' '\\r' '\\n' 'S' 't' '.' ' ' 'P' 'e' 't'\n",
            " 'e' 'r' 's' 'b' 'u' 'r' 'g' 'h' ',' ' ' 'D' 'e' 'c' '.' ' ' '1' '1' 't'\n",
            " 'h' ',' ' ' '1' '7' '.' '\\r' '\\n' '\\r' '\\n' '\\r' '\\n' 'Y' 'o' 'u' ' ' 'w'\n",
            " 'i' 'l' 'l' ' ' 'r' 'e' 'j' 'o' 'i' 'c' 'e' ' ' 't' 'o']\n",
            "[' ' 'h' 'e' 'a' 'r' ' ' 't' 'h' 'a' 't' ' ' 'n' 'o' ' ' 'd' 'i' 's' 'a'\n",
            " 's' 't' 'e' 'r' ' ' 'h' 'a' 's' ' ' 'a' 'c' 'c' 'o' 'm' 'p' 'a' 'n' 'i'\n",
            " 'e' 'd' ' ' 't' 'h' 'e' '\\r' '\\n' 'c' 'o' 'm' 'm' 'e' 'n' 'c' 'e' 'm' 'e'\n",
            " 'n' 't' ' ' 'o' 'f' ' ' 'a' 'n' ' ' 'e' 'n' 't' 'e' 'r' 'p' 'r' 'i' 's'\n",
            " 'e' ' ' 'w' 'h' 'i' 'c' 'h' ' ' 'y' 'o' 'u' ' ' 'h' 'a' 'v' 'e' ' ' 'r'\n",
            " 'e' 'g' 'a' 'r' 'd' 'e' 'd' ' ' 'w' 'i' 't']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHUDyNSjBYFP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73e8de00-6b21-41ab-c71a-030fbe10c618"
      },
      "source": [
        "def show_str(item, prefix=None):\n",
        "    if prefix is not None:\n",
        "       print(prefix)\n",
        "    print(\"\".join(idx2char[item]))\n",
        "\n",
        "# Visualize first 3 sequences\n",
        "for i,v in sequences.enumerate().take(3):\n",
        "    show_str(v, \"Sequence {}:\".format(i))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequence 0:\n",
            "Letter 1\r\n",
            "\r\n",
            "_To Mrs. Saville, England._\r\n",
            "\r\n",
            "\r\n",
            "St. Petersburgh, Dec. 11th, 17.\r\n",
            "\r\n",
            "\r\n",
            "You will rejoice to\n",
            "Sequence 1:\n",
            " hear that no disaster has accompanied the\r\n",
            "commencement of an enterprise which you have regarded wit\n",
            "Sequence 2:\n",
            "h such evil\r\n",
            "forebodings.  I arrived here yesterday, and my first task is to assure\r\n",
            "my dear sister o\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rk2sYxF4Eo-G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0abdd87d-547f-4c86-d9b9-8546b532d3c1"
      },
      "source": [
        "# Define splitting method\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "# Test splitting method\n",
        "print(split_input_target(\"Test phrase\"))\n",
        "\n",
        "# Creating dataset from sequences with our splitting method\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Test phras', 'est phrase')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YoqZfOaE153",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b222c26-d842-47ea-e3a9-193bb064db49"
      },
      "source": [
        "for input_example, target_example in dataset.take(2):\n",
        "    show_str (input_example, 'Input data: ')\n",
        "    show_str (target_example, 'Target data: ')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data: \n",
            "Letter 1\r\n",
            "\r\n",
            "_To Mrs. Saville, England._\r\n",
            "\r\n",
            "\r\n",
            "St. Petersburgh, Dec. 11th, 17.\r\n",
            "\r\n",
            "\r\n",
            "You will rejoice t\n",
            "Target data: \n",
            "etter 1\r\n",
            "\r\n",
            "_To Mrs. Saville, England._\r\n",
            "\r\n",
            "\r\n",
            "St. Petersburgh, Dec. 11th, 17.\r\n",
            "\r\n",
            "\r\n",
            "You will rejoice to\n",
            "Input data: \n",
            " hear that no disaster has accompanied the\r\n",
            "commencement of an enterprise which you have regarded wi\n",
            "Target data: \n",
            "hear that no disaster has accompanied the\r\n",
            "commencement of an enterprise which you have regarded wit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wPX-QXpE8T3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85f562cf-1c85-4091-d253-3a3bc0df1cfd"
      },
      "source": [
        "# Take first dataset string pair again\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    pass\n",
        "# Show symbols for each time step as illustration\n",
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, idx2char[input_idx]))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, idx2char[target_idx]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 33 (L)\n",
            "  expected output: 52 (e)\n",
            "Step    1\n",
            "  input: 52 (e)\n",
            "  expected output: 67 (t)\n",
            "Step    2\n",
            "  input: 67 (t)\n",
            "  expected output: 67 (t)\n",
            "Step    3\n",
            "  input: 67 (t)\n",
            "  expected output: 52 (e)\n",
            "Step    4\n",
            "  input: 52 (e)\n",
            "  expected output: 65 (r)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SCCer7aE_f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c74439d2-cf5c-4445-d86c-5f25e8f90cfb"
      },
      "source": [
        "# Check dataset classes\n",
        "print(\"dataset class is {}\".format(dataset))\n",
        "print(\"dataset is a child of tf.data.Dataset? {}.\".format(isinstance(dataset, tf.data.Dataset)))\n",
        "\n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset_batched = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "print(\"dataset_batched is new variable of class {}.\".format(dataset_batched))\n",
        "print(\"dataset_batched is a child of tf.data.Dataset? {}.\".format(isinstance(dataset_batched, tf.data.Dataset)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset class is <MapDataset shapes: ((100,), (100,)), types: (tf.uint8, tf.uint8)>\n",
            "dataset is a child of tf.data.Dataset? True.\n",
            "dataset_batched is new variable of class <BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.uint8, tf.uint8)>.\n",
            "dataset_batched is a child of tf.data.Dataset? True.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nduES9hcFDov",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7da9a674-5c3e-4b75-db72-dc6c61e93ee3"
      },
      "source": [
        "# Take first dataset batch as a pair\n",
        "for input_batch, target_batch in dataset_batched.take(1):\n",
        "    pass\n",
        "\n",
        "# You can check that we have 2D array with the following debug output\n",
        "print(\"input_batch is new variable of class {}.\".format(input_batch))\n",
        "print(\"So dataset_batched can be converted to a pair of arrays with shape {} and {}.\".format(input_batch.shape, target_batch.shape))\n",
        "\n",
        "# Let's look into some strings\n",
        "for i in range(2):\n",
        "    input_example = input_batch[i]\n",
        "    target_example = target_batch[i]\n",
        "    print(\"String pair #{}\".format(i))\n",
        "    show_str(input_example, \"Input sequence:\")\n",
        "    show_str(target_example, \"Target sequence:\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_batch is new variable of class [[55  6  2 ... 68 60 52]\n",
            " [72  2 62 ... 55  2 30]\n",
            " [67 55 52 ... 53 59 68]\n",
            " ...\n",
            " [ 2 37 48 ... 67 61 56]\n",
            " [65 52 63 ... 55 52  2]\n",
            " [52 65 52 ... 58  2 62]].\n",
            "So dataset_batched can be converted to a pair of arrays with shape (64, 100) and (64, 100).\n",
            "String pair #0\n",
            "Input sequence:\n",
            "h, cried she, that I were to die with you; I\r\n",
            "cannot live in this world of misery.\r\n",
            "\r\n",
            "Justine assume\n",
            "Target sequence:\n",
            ", cried she, that I were to die with you; I\r\n",
            "cannot live in this world of misery.\r\n",
            "\r\n",
            "Justine assumed\n",
            "String pair #1\n",
            "Input sequence:\n",
            "y of the seas rather than abandon my\r\n",
            "purpose. I hoped to induce you to grant me a boat with which I\n",
            "Target sequence:\n",
            " of the seas rather than abandon my\r\n",
            "purpose. I hoped to induce you to grant me a boat with which I \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWdbZh6-FIbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3ec43de-9ff1-460d-c8fa-fccb7eed7bb6"
      },
      "source": [
        "for input_batch, target_batch in dataset_batched.take(1):\n",
        "    pass\n",
        "tf.one_hot(input_batch, len(vocab))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(64, 100, 74), dtype=float32, numpy=\n",
              "array([[[0., 0., 1., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 1., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 1., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 1., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 1., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 1., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 1., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 1., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGJUk_VQFOOP"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Number of RNN units\n",
        "lstm_units = 1024\n",
        "\n",
        "def build_model(vocab_size, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    # To call tf.one_hot() we use Lambda, but we also have to cast type to uint8 \n",
        "    # because otherwise model weights can't be imported as they do not support default float32 type.\n",
        "    # We also have to define batch shape because model can't be used wighout knowing its input shape.\n",
        "    # It can be defined on first use, but it is more simple to define it now.\n",
        "    tf.keras.layers.Lambda(lambda x: tf.one_hot(tf.cast(x, 'uint8'), vocab_size), \n",
        "                           batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwBgGWJvFT4X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edd2fa3e-d00f-4821-fef7-f1a59e383d1a"
      },
      "source": [
        "model_batched = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  rnn_units=lstm_units,\n",
        "  batch_size=BATCH_SIZE)\n",
        "\n",
        "model_batched.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda (Lambda)              (64, None, 74)            0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (64, None, 1024)          4501504   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 74)            75850     \n",
            "=================================================================\n",
            "Total params: 4,577,354\n",
            "Trainable params: 4,577,354\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gigBPtXmFYp-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e052c62-5202-4215-dcc7-c9402479bb1f"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset_batched.take(1):\n",
        "  print(\"Input shape \", input_example_batch.shape, \"# (batch_size, sequence_length)\")\n",
        "  print(\"Target shape \", target_example_batch.shape, \"# (batch_size, sequence_length)\")\n",
        "  example_batch_predictions = model_batched(input_example_batch)\n",
        "  print(\"Prediction shape \", example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input shape  (64, 100) # (batch_size, sequence_length)\n",
            "Target shape  (64, 100) # (batch_size, sequence_length)\n",
            "Prediction shape  (64, 100, 74) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH1nSntWFcfu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fce3abcb-47e3-4a85-aff0-b67d90c6cc61"
      },
      "source": [
        "sampled_indices = tf.argmax(example_batch_predictions[0], axis=1)\n",
        "print(sampled_indices)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[18 21 40 68 73 73 73 73 73 71 71 71 71 65 65 65 65 65 21 71 71 71 71 42\n",
            " 64  4  4  4  4  4  4  3 27 15 62 15 22 48 48 48 48 22 48 71  4 48 40 71\n",
            " 71  4 48 22 22 15  4 68 22 68 68 71 71 71 10 10 71 16 16 16 72 72 65  6\n",
            " 72 70 48 66 48 48 48 48 48 48 48 48 48 48 48 48 48  3 48 48 71 42 22  4\n",
            "  4  4 42 42], shape=(100,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxwOPTITFfWf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf8846e2-2af3-4cd0-80b7-1ad013200ade"
      },
      "source": [
        "show_str(input_example_batch[0], \"Input sequence:\")\n",
        "show_str(target_example_batch[0], \"Target sequence:\")\n",
        "show_str(sampled_indices, \"Predicted sequence:\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input sequence:\n",
            "ch perpetually increased, I\r\n",
            "brought my work near to a conclusion.\r\n",
            "\r\n",
            "The summer months passed while\n",
            "Target sequence:\n",
            "h perpetually increased, I\r\n",
            "brought my work near to a conclusion.\r\n",
            "\r\n",
            "The summer months passed while \n",
            "Predicted sequence:\n",
            "9?Tuzzzzzxxxxrrrrr?xxxxVq((((((!F6o6AaaaaAax(aTxx(aAA6(uAuuxxx11x777yyr,ywasaaaaaaaaaaaaa!aaxVA(((VV\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXUHFEuZFk_S"
      },
      "source": [
        "model_batched.compile(optimizer='adam', loss=tf.keras.losses.sparse_categorical_crossentropy)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCLAf3szFn9p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99d4b159-c7e7-4250-be66-3d6cd0be46e1"
      },
      "source": [
        "example_batch_loss  = tf.keras.losses.sparse_categorical_crossentropy(target_example_batch, example_batch_predictions)\n",
        "print(\"scalar_loss: \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "scalar_loss:  4.304465\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L5GdYwrFrxJ"
      },
      "source": [
        "EPOCHS=10"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sOmb7b5FsTL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e30e1f2-a1a2-41fc-c927-45076a35c08e"
      },
      "source": [
        "history = model_batched.fit(dataset_batched, epochs=EPOCHS)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "66/66 [==============================] - 382s 6s/step - loss: 3.1083\n",
            "Epoch 2/10\n",
            "66/66 [==============================] - 377s 6s/step - loss: 2.7136\n",
            "Epoch 3/10\n",
            "66/66 [==============================] - 382s 6s/step - loss: 2.3484\n",
            "Epoch 4/10\n",
            "66/66 [==============================] - 378s 6s/step - loss: 2.1871\n",
            "Epoch 5/10\n",
            "66/66 [==============================] - 377s 6s/step - loss: 2.0898\n",
            "Epoch 6/10\n",
            "66/66 [==============================] - 379s 6s/step - loss: 1.9932\n",
            "Epoch 7/10\n",
            "66/66 [==============================] - 378s 6s/step - loss: 1.9112\n",
            "Epoch 8/10\n",
            "66/66 [==============================] - 377s 6s/step - loss: 1.8327\n",
            "Epoch 9/10\n",
            "66/66 [==============================] - 379s 6s/step - loss: 1.7619\n",
            "Epoch 10/10\n",
            "66/66 [==============================] - 383s 6s/step - loss: 1.6916\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrwcPEfEFwY1"
      },
      "source": [
        "model_weights = model_batched.get_weights()\n",
        "\n",
        "model_single = build_model(vocab_size, lstm_units, batch_size=1)\n",
        "\n",
        "model_single.set_weights(model_weights)\n",
        "\n",
        "model_single.build()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzE-g9ywFyY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9de7d56-ef35-4192-90c5-9ed67075ea23"
      },
      "source": [
        "model_single.summary()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda_1 (Lambda)            (1, None, 74)             0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (1, None, 1024)           4501504   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 74)             75850     \n",
            "=================================================================\n",
            "Total params: 4,577,354\n",
            "Trainable params: 4,577,354\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eV1NzGn1F1N3"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = np.array([char2idx[s] for s in start_string])\n",
        "  print(\"input_eval original shape is \", input_eval.shape)\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "  print(\"input_eval shape changed to \", input_eval.shape)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Here batch size == 1\n",
        "  # We reset the memory of RNN so it will forget the previous timeline os characters sequence\n",
        "  model.reset_states()\n",
        "    \n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # print(\"Predictions shape before squeezing \", predictions.shape)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "      # print(\"Predictions shape after squeezing \", predictions.shape)\n",
        "\n",
        "      # using argmax() to determine next symbol\n",
        "      #predicted_id = tf.argmax(predictions[0])\n",
        "      temperature = 0.021\n",
        "      predicted_id = tf.random.categorical(predictions / temperature, num_samples=1)[-1,0].numpy()\n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JcoBlx7F3ZH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5b93d8d-e942-4a92-d017-f6f508a3846b"
      },
      "source": [
        "print(generate_text(model_single, start_string=u\"We are on the verge of \"))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_eval original shape is  (23,)\n",
            "input_eval shape changed to  (1, 23)\n",
            "We are on the verge of the sure of the reather of the and and the some of the nother of the hares of the ine of the some of the was and the some of the reather of the soon of the hares of the count of the some that I had destroyed the soon of the count of the forth of the was and and and the forth of the more of the (are of the some of the and and and the )ore of the 6reat of the reather of the and and and the some to my eres.  I sad to the dear would not the some of the [ore of the dear bean sould and the some of the ore of the some of the count of the more of the dear some the xire of the reather of the quisted of the forth of the hares of the count of the dear to and and the soon of the companion of the tore of the reather of the sure of the are of the Rome of the forth of the reather of the forth of the some of the [ore of the the look of the tore of the reather of the companion of the count of the some that I had been the Wallent of the hares of the ore of the and and the coust of the count of the nothe\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx8B82L9F-Ci"
      },
      "source": [
        "Эксперимент 1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x1gq-19F_Y_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73b2e85e-0cd3-4570-8359-cce24afbff15"
      },
      "source": [
        "num_of_samples = 200\n",
        "probabilities = np.array([[0.5, 0.2, 0.2, 0.1]])\n",
        "temperature = 1\n",
        "\n",
        "samples = tf.random.categorical(tf.math.log(probabilities) / temperature, num_of_samples)\n",
        "print(samples)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[0 2 0 1 0 0 0 0 0 2 0 0 0 0 2 1 2 3 3 0 0 0 2 3 0 2 2 1 0 0 0 2 2 0 0 1\n",
            "  0 0 2 2 3 0 0 2 2 0 0 0 3 3 1 3 1 0 0 2 0 0 2 0 1 3 0 2 1 3 2 0 2 2 1 0\n",
            "  0 0 0 0 0 2 0 0 0 0 2 2 0 1 2 0 2 1 3 2 2 0 0 2 0 0 0 2 0 3 0 2 0 0 0 0\n",
            "  3 2 0 2 2 1 0 1 0 0 0 2 3 0 0 0 0 0 0 0 0 0 2 0 2 2 0 0 0 0 0 0 0 0 0 2\n",
            "  0 0 0 1 3 2 1 0 1 0 0 3 2 0 0 0 1 0 1 0 3 3 1 2 0 1 0 0 1 2 0 1 2 0 1 2\n",
            "  3 3 1 2 0 0 0 0 0 0 1 1 2 0 3 0 3 2 0 0]], shape=(1, 200), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDz4RfWaGFe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "8f32c986-89fa-454b-f003-080011785e43"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# the histogram of the data\n",
        "_, bins, _ = plt.hist(samples, bins=len(probabilities[0]), \n",
        "                            density=False, color='lawngreen', rwidth = 0.8)\n",
        "# Probability lines\n",
        "for p in probabilities[0]:\n",
        "    plt.plot(bins, [p*num_of_samples for _ in bins], color='b' )\n",
        "plt.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}